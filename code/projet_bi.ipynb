{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET BI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.bil.com/search?find_desc=paris+new+york&find_loc=Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv('inspections-restaurants.csv',sep=';')\n",
    "arrond=np.unique(data['Code postal'])[:17]\n",
    "paris=data[(data['Code postal']/1000).astype(int)==75]\n",
    "#paris['Evaluation'].astype(int).mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_urls():\n",
    "    data=pd.read_csv('inspections-restaurants.csv',sep=';')\n",
    "    names=[(n.lower()).replace(' ','+') for n in data['Nom']]\n",
    "    loc=[loc for loc in data['Code commune']]\n",
    "    path='https://www.yelp.com/search?'\n",
    "    urls=[]\n",
    "    for i in range(len(data)):\n",
    "        urls.append(path+'find_desc='+names[i]+'&find_loc='+str(loc[i]))\n",
    "    return urls\n",
    "\n",
    "def get_bil_urls():\n",
    "    data=pd.read_csv('inspections-restaurants.csv',sep=';')\n",
    "    siret=data['SIRET'].values\n",
    "    urls=[]\n",
    "    for s in siret:\n",
    "        url='http://www.bilansgratuits.fr/entreprise/fiche/'+str(s)+'.htm'\n",
    "        siret=s\n",
    "        urls.append((url,siret))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import unicodedata\n",
    "from BeautifulSoup import BeautifulStoneSoup\n",
    "\n",
    "def process(txt):\n",
    "    #txt = txt[txt.find(\"\\n\\n\"):] # elimination de l'entete (on ne conserve que les caractères après la première occurence du motif\n",
    "    txt = unicodedata.normalize(\"NFKD\",txt).encode(\"utf-8\",\"ignore\") # elimination des caractères spéciaux, accents...\n",
    "    punc = string.punctuation    # recupération de la ponctuation\n",
    "    punc += u'\\n\\r\\t\\\\'          # ajouts de caractères à enlever\n",
    "    table =string.maketrans(punc, ' '*len(punc))  # table de conversion punc -> espace\n",
    "    txt = string.translate(txt,table).lower() # elimination des accents + minuscules\n",
    "    return txt\n",
    "\n",
    "def conv_html(htmlstring):\n",
    "    return BeautifulStoneSoup(htmlstring,\n",
    "                   convertEntities=BeautifulStoneSoup.HTML_ENTITIES).contents[0]\n",
    "\n",
    "\n",
    "yelp=pd.read_csv('yelp_revs.csv',sep=';',encoding='utf-8')\n",
    "#print process(conv_html(yelp['comment'].loc[2]))\n",
    "yelp['revs']=yelp['comment'].apply(lambda s: process(conv_html(s)))\n",
    "\n",
    "y=yelp[yelp['revs'].str.contains('sale')]['revs']\n",
    "yb=[yi for yi in y if(not np.where('sale' in yi))]\n",
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## etalages_et_terrasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dossier</th>\n",
       "      <th>arrondissement</th>\n",
       "      <th>adresse</th>\n",
       "      <th>lieu</th>\n",
       "      <th>libelle_type_objet</th>\n",
       "      <th>dima</th>\n",
       "      <th>dimb</th>\n",
       "      <th>dosred</th>\n",
       "      <th>Coordonnées</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>793 937 093 002</td>\n",
       "      <td>75020</td>\n",
       "      <td>93 AVENUE GAMBETTA</td>\n",
       "      <td>A</td>\n",
       "      <td>TERRASSE OUVERTE</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>SARL CEP 20  (BRASSERIE)</td>\n",
       "      <td>48.866456, 2.399652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>793 937 125 003</td>\n",
       "      <td>75020</td>\n",
       "      <td>Pan coupé 125 AVENUE GAMBETTA / 120 RUE ORFILA</td>\n",
       "      <td>A</td>\n",
       "      <td>TERRASSE OUVERTE</td>\n",
       "      <td>2.67</td>\n",
       "      <td>4.26</td>\n",
       "      <td>SARL LA POINTE AUX PIMENTS  (RESTAURANT)</td>\n",
       "      <td>48.868115, 2.40109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>793 939 002 002</td>\n",
       "      <td>75020</td>\n",
       "      <td>202 RUE DES PYRENEES</td>\n",
       "      <td>B</td>\n",
       "      <td>TERRASSE FERMEE</td>\n",
       "      <td>1.30</td>\n",
       "      <td>12.05</td>\n",
       "      <td>SNC INDIANA GAMBETTA  (RESTAURANT)</td>\n",
       "      <td>48.864386, 2.39905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>793 939 007 003</td>\n",
       "      <td>75020</td>\n",
       "      <td>7 PLACE GAMBETTA</td>\n",
       "      <td>A</td>\n",
       "      <td>TERRASSE OUVERTE</td>\n",
       "      <td>0.60</td>\n",
       "      <td>8.20</td>\n",
       "      <td>SA MC DONALD S FRANCE  (RESTAURATION RAPIDE - ...</td>\n",
       "      <td>48.864778, 2.397904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>793 939 010 002</td>\n",
       "      <td>75020</td>\n",
       "      <td>206 RUE DES PYRENES</td>\n",
       "      <td>B</td>\n",
       "      <td>TERRASSE OUVERTE</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.30</td>\n",
       "      <td>M ROMIEU  (BRASSERIE)</td>\n",
       "      <td>48.865147, 2.398322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dossier  arrondissement  \\\n",
       "0  793 937 093 002           75020   \n",
       "1  793 937 125 003           75020   \n",
       "2  793 939 002 002           75020   \n",
       "3  793 939 007 003           75020   \n",
       "4  793 939 010 002           75020   \n",
       "\n",
       "                                          adresse lieu libelle_type_objet  \\\n",
       "0                              93 AVENUE GAMBETTA    A   TERRASSE OUVERTE   \n",
       "1  Pan coupé 125 AVENUE GAMBETTA / 120 RUE ORFILA    A   TERRASSE OUVERTE   \n",
       "2                            202 RUE DES PYRENEES    B    TERRASSE FERMEE   \n",
       "3                                7 PLACE GAMBETTA    A   TERRASSE OUVERTE   \n",
       "4                             206 RUE DES PYRENES    B   TERRASSE OUVERTE   \n",
       "\n",
       "   dima   dimb                                             dosred  \\\n",
       "0  1.50   2.00                           SARL CEP 20  (BRASSERIE)   \n",
       "1  2.67   4.26           SARL LA POINTE AUX PIMENTS  (RESTAURANT)   \n",
       "2  1.30  12.05                 SNC INDIANA GAMBETTA  (RESTAURANT)   \n",
       "3  0.60   8.20  SA MC DONALD S FRANCE  (RESTAURATION RAPIDE - ...   \n",
       "4  0.70   3.30                              M ROMIEU  (BRASSERIE)   \n",
       "\n",
       "           Coordonnées  \n",
       "0  48.866456, 2.399652  \n",
       "1   48.868115, 2.40109  \n",
       "2   48.864386, 2.39905  \n",
       "3  48.864778, 2.397904  \n",
       "4  48.865147, 2.398322  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "etal=pd.read_csv('etalages_et_terrasses_autorisees_a_paris.csv',sep=';')\n",
    "etal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ajout d'une nouvelle colonne indiquant le libelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etal['libelle']=etal.dosred.apply(lambda s:s[s.find(\"(\")+1:s.find(\")\")]).values\n",
    "etal['DOSRED_MOD']=etal.dosred.apply(lambda s:s[:s.find(\"(\")]+s[s.find(\")\")+1:]).values\n",
    "#etal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des restaurants uniquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extraction des restaurants parmi toutes les données\n",
    "restos=etal[etal['dosred'].str.contains('RESTA')]\n",
    "brass=etal[etal['dosred'].str.contains('BRASSERIE')]\n",
    "pizz=etal[etal['dosred'].str.contains('PIZZERIA')]\n",
    "all_rest=restos.append(brass).append(pizz)\n",
    "all_rest.to_csv('etal_rests.csv',sep=';')\n",
    "\n",
    "all_rest=all_rest[~all_rest['Coordonnées'].isnull()]\n",
    "\n",
    "sanit=pd.read_csv('inspections-restaurants.csv',sep=';')\n",
    "sanit.head()\n",
    "sanit['Coordonnées géographiques']=sanit['Coordonnées géographiques']=sanit['Coordonnées géographiques'].apply(lambda x: np.array(x.split(','),dtype='float'))\n",
    "all_rest['Coordonnées']=all_rest['Coordonnées'].apply(lambda x: np.array(x.split(','),dtype='float'))\n",
    "sanit['Coordonnées géographiques']\n",
    "all_rest['dist']=99999.\n",
    "loc=4\n",
    "a=sanit['Coordonnées géographiques'].loc[loc]\n",
    "#print a\n",
    "#dist=[np.linalg.norm(x-a) for x in all_rest['Coordonn\\xc3\\xa9es']]\n",
    "#m,argm=np.min(dist),np.argmin(dist)\n",
    "#print m\n",
    "#print sanit.loc[loc]\n",
    "#print ''\n",
    "#print all_rest.loc[argm]\n",
    "#all_rest['dist']=all_rest['dist'].apply(lambda x:np.linalg.norm(a-x))\n",
    "#all_rest[:20]\n",
    "#np.linalg.norm()\n",
    "#for i in range(len(sanit)):\n",
    "    #coord=sanit['Coordonnées géographiques'].loc[i]\n",
    "#    a,b=sanit['Coordonnées géographiques'].values[i].split(',')\n",
    "#    print ','.join([a[:9],b[:8]])\n",
    "#    pos=','.join([a[:9],b[:8]])\n",
    "#    print all_rest[all_rest['Coordonnées'].str.contains(pos)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web crawler pour extraire les bilans à partir du n de SIRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "from urlparse import urljoin\n",
    "import urllib2\n",
    "import argparse\n",
    "import re\n",
    "import codecs\n",
    "import time\n",
    "import random\n",
    "import biurls\n",
    "import pandas as pd\n",
    "\n",
    "get_bil_page = lambda page: biurls.get_bil_urls()[page][0]\n",
    "\n",
    "\n",
    "def crawl_page(page, verbose=False):\n",
    "\n",
    "    try:\n",
    "        page_url = get_bil_page(page)\n",
    "        soup = BeautifulSoup(urllib2.urlopen(page_url).read())\n",
    "\n",
    "    except Exception, e:\n",
    "        print str(e)\n",
    "        return []\n",
    "    data={}\n",
    "    rest=soup.findAll('h2',attrs={'class':\"title accroche\"},text='Informations Générales'.decode('utf-8'))\n",
    "    rest=rest[0].next.next\n",
    "    fj=''\n",
    "    cap=''\n",
    "    nbetab=''\n",
    "    date=''\n",
    "    data['url']=page_url\n",
    "    try:\n",
    "        fj=rest.find('td',{'class':'leftElement large1'},text='Forme juridique').next.next.next\n",
    "        data['forme juridique']=fj\n",
    "    except Exception, e:\n",
    "        if verbose: print 'extract fail', str(e)\n",
    "    try:\n",
    "        cap=rest.find('td',{'class':'leftElement large1'},text='Capital').next.next.next\n",
    "        cap=cap.split('&nbsp;&euro;')[0]\n",
    "        data['capital']=cap\n",
    "    except Exception, e:\n",
    "        if verbose: print 'extract fail', str(e)\n",
    "    try:\n",
    "        nbetab=rest.find('td',{'class':'leftElement large1'},text='Nombre d\\'établissements'.decode('utf-8')).next.next.next\n",
    "        data['nombre etabl']=nbetab\n",
    "    except Exception, e:\n",
    "        if verbose: print 'extract fail', str(e)\n",
    "    try:\n",
    "        date=rest.find('td',{'class':'leftElement large1'},text='Création'.decode('utf-8')).next.next.next\n",
    "        data['date creation']=date\n",
    "    except Exception, e:\n",
    "        if verbose: print 'extract fail', str(e)\n",
    "\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "urls=biurls.get_bil_urls()\n",
    "results=[]\n",
    "for index in range(len(urls)):\n",
    "    print('index: '+str(index))\n",
    "    rest=crawl_page(index)\n",
    "    rest['page']=index\n",
    "    rest['siret']=urls[index][1]\n",
    "    results.append(rest)\n",
    "    #print rest\n",
    "    print 'page crawled'\n",
    "    time.sleep(random.randint(1, 3) * .931467298)\n",
    "df=pd.DataFrame(results,index_col=False)\n",
    "df.to_csv('siret.csv',sep=';',index_col=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(results)\n",
    "df.to_csv('siret.csv',sep=';',index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For google Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paris_rest=data[(data['Code postal']/1000).astype(int)==84]\n",
    "pr=paris_rest[['Coordonnées géographiques','Nom','Evaluation']]\n",
    "res=[]\n",
    "for i in range(len(pr)):\n",
    "    s=pr.values[i]\n",
    "    s[1]=s[1].replace('\\'','\\\\\\'')\n",
    "    res.append('['+s[0]+',\\''+s[1]+', Evalution: '+str(s[2])+'\\''+']')\n",
    "print ',\\n'.join(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
